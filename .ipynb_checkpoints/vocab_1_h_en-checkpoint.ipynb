{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a615600",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "778ee3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\envs\\cits4012\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7adcc2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3fb0b29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_type</th>\n",
       "      <th>short_description</th>\n",
       "      <th>job_type_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consulting &amp; Strategy</td>\n",
       "      <td>['lead', 'and', 'develop', 'team', 'of', 'pric...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['and', 'develop', 'of', 'pricing', 'and', 'pr...</td>\n",
       "      <td>Full Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>['backed', 'by', 'the', 'australian', 'chamber...</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>['the', 'the', 'and', 'and', 'and', 'the', 'of...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>['boeing', 'defence', 'australia', 'ltd', 'who...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['boeing', 'defence', 'of', 'the', 'boeing', '...</td>\n",
       "      <td>Full Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hospitality &amp; Tourism</td>\n",
       "      <td>['restaurant', 'in', 'runaway', 'bay', 'is', '...</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>['runaway', 'bay', 'casual', 'cook', 'to', 'to...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Community Services &amp; Development</td>\n",
       "      <td>['about', 'the', 'company', 'apm', 'is', 'an',...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['the', 'apm', 'with', 'and', 'apm', 'employme...</td>\n",
       "      <td>Full Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>Banking &amp; Financial Services</td>\n",
       "      <td>['about', 'the', 'company', 'our', 'client', '...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['about', 'the', 'company', 'track', 'record',...</td>\n",
       "      <td>Full Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>Information &amp; Communication Technology</td>\n",
       "      <td>['data', 'centre', 'support', 'engineer', 'mon...</td>\n",
       "      <td>Contract/Temp</td>\n",
       "      <td>['data', 'centre', 'support', 'telco', 'cloud'...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>Hospitality &amp; Tourism</td>\n",
       "      <td>['new', 'business', 'freshly', 'renovated', 'o...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['freshly', 'renovated', 'par', 'steak', 'insi...</td>\n",
       "      <td>Full Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>['downer', 'construction', 'business', 'has', ...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['downer', 'construction', 'bauxite', 'mine', ...</td>\n",
       "      <td>Full Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>Call Centre &amp; Customer Service</td>\n",
       "      <td>['cash', 'this', 'is', 'position', 'offering',...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['cash', 'is', 'to', 'and', 'we', 'we', 'we', ...</td>\n",
       "      <td>Full Time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2963 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    category  \\\n",
       "0                      Consulting & Strategy   \n",
       "1                          Trades & Services   \n",
       "2                                 Accounting   \n",
       "3                      Hospitality & Tourism   \n",
       "4           Community Services & Development   \n",
       "...                                      ...   \n",
       "2958            Banking & Financial Services   \n",
       "2959  Information & Communication Technology   \n",
       "2960                   Hospitality & Tourism   \n",
       "2961                       Trades & Services   \n",
       "2962          Call Centre & Customer Service   \n",
       "\n",
       "                                        job_description         job_type  \\\n",
       "0     ['lead', 'and', 'develop', 'team', 'of', 'pric...        Full Time   \n",
       "1     ['backed', 'by', 'the', 'australian', 'chamber...  Casual/Vacation   \n",
       "2     ['boeing', 'defence', 'australia', 'ltd', 'who...        Full Time   \n",
       "3     ['restaurant', 'in', 'runaway', 'bay', 'is', '...  Casual/Vacation   \n",
       "4     ['about', 'the', 'company', 'apm', 'is', 'an',...        Full Time   \n",
       "...                                                 ...              ...   \n",
       "2958  ['about', 'the', 'company', 'our', 'client', '...        Full Time   \n",
       "2959  ['data', 'centre', 'support', 'engineer', 'mon...    Contract/Temp   \n",
       "2960  ['new', 'business', 'freshly', 'renovated', 'o...        Full Time   \n",
       "2961  ['downer', 'construction', 'business', 'has', ...        Full Time   \n",
       "2962  ['cash', 'this', 'is', 'position', 'offering',...        Full Time   \n",
       "\n",
       "                                      short_description job_type_target  \n",
       "0     ['and', 'develop', 'of', 'pricing', 'and', 'pr...       Full Time  \n",
       "1     ['the', 'the', 'and', 'and', 'and', 'the', 'of...           Other  \n",
       "2     ['boeing', 'defence', 'of', 'the', 'boeing', '...       Full Time  \n",
       "3     ['runaway', 'bay', 'casual', 'cook', 'to', 'to...           Other  \n",
       "4     ['the', 'apm', 'with', 'and', 'apm', 'employme...       Full Time  \n",
       "...                                                 ...             ...  \n",
       "2958  ['about', 'the', 'company', 'track', 'record',...       Full Time  \n",
       "2959  ['data', 'centre', 'support', 'telco', 'cloud'...           Other  \n",
       "2960  ['freshly', 'renovated', 'par', 'steak', 'insi...       Full Time  \n",
       "2961  ['downer', 'construction', 'bauxite', 'mine', ...       Full Time  \n",
       "2962  ['cash', 'is', 'to', 'and', 'we', 'we', 'we', ...       Full Time  \n",
       "\n",
       "[2963 rows x 5 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b2cc0",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b727444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existingmap of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                                for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = 1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "        or the UNK index if token isn't present.\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "            for the UNK functionality\n",
    "        \"\"\"\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "        KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11374c4",
   "metadata": {},
   "source": [
    "## Sequence Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "418b5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for glove encoder\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aaa275",
   "metadata": {},
   "source": [
    "## One-hot-encoding Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50f088c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "class One_hot_Vectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, description_vocab, target_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description_vocab (Vocabulary): maps words to integers\n",
    "            target_vocab (Vocabulary): maps class labels to integers\n",
    "        \"\"\"\n",
    "        self.description_vocab = description_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "    def vectorize(self, description_tokenized):\n",
    "        \"\"\"Create a collapsed one hot vector for the job description\n",
    "        Args:\n",
    "            description_tokenized (list): the tokenized job description\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed onehot encoding\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.description_vocab), dtype=np.float32)\n",
    "        for token in description_tokenized:\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.description_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, description_df, target_cat = True, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        Args:\n",
    "            description_df (pandas.DataFrame): the description dataset\n",
    "            cutoff (int): the parameter for frequency based filtering\n",
    "        Returns:\n",
    "            an instance of the descriptionVectorizer\n",
    "        \"\"\"\n",
    "        description_vocab = Vocabulary(add_unk=True)\n",
    "        target_vocab = Vocabulary(add_unk=False)\n",
    "        # adding category or Job_type\n",
    "        if target_cat:\n",
    "            for category in sorted(set(description_df.category)):\n",
    "                target_vocab.add_token(category)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for job_type in  sorted(set(description_df.job_type_target)):\n",
    "                target_vocab.add_token(job_type)\n",
    "        # Add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for description in description_df.job_description:\n",
    "            for word in description:\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                description_vocab.add_token(word)\n",
    "        return cls(description_vocab, target_vocab)\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Intantiate a descriptionVectorizer from a serializable dictionary\n",
    "        Args:\n",
    "            contents (dict): the serializable dictionary\n",
    "        Returns:\n",
    "            an instance of the descriptionVectorizer class\n",
    "        \"\"\"\n",
    "        description_vocab = Vocabulary.from_serializable(contents['description_vocab'])\n",
    "        target_vocab = Vocabulary.from_serializable(contents['target_vocab'])\n",
    "        return cls(description_vocab=description_vocab, target_vocab=target_vocab)\n",
    "    def to_serializable(self):\n",
    "        \"\"\"Create the serializable dictionary for caching\n",
    "        Returns:\n",
    "            contents (dict): the serializable dictionary\n",
    "        \"\"\"\n",
    "        return {'description_vocab': self.description_vocab.to_serializable(),\n",
    "                'target_vocab': self.target_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c6286218",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder_job_type = One_hot_Vectorizer.from_dataframe(df, target_cat = False)\n",
    "one_hot_encoder_cat = One_hot_Vectorizer.from_dataframe(df, target_cat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cdc8bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"one_hot_encoder_job_type.json\", \"w\") as fp:\n",
    "            json.dump(one_hot_encoder_job_type.to_serializable(), fp)\n",
    "        \n",
    "with open(\"one_hot_encoder_cat.json\", \"w\") as fp:\n",
    "            json.dump(one_hot_encoder_cat.to_serializable(), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da678990",
   "metadata": {},
   "source": [
    "## Glove vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7730e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, description_vocab, target_vocab):\n",
    "        self.description_vocab = description_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def vectorize(self, description, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description (list) : tokenized description \n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized description (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = [self.description_vocab.begin_seq_index]\n",
    "        indices.extend(self.description_vocab.lookup_token(token) \n",
    "                       for token in description.split(\" \"))\n",
    "        indices.append(self.description_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.description_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, description_df, target_cat = True, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            description_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the descriptionVectorizer\n",
    "        \"\"\"\n",
    "        target_vocab = Vocabulary()\n",
    "        if target_cat:\n",
    "            for target in sorted(set(description_df.category)):\n",
    "                target_vocab.add_token(target)\n",
    "        \n",
    "        else:\n",
    "            for job_type in sorted(set(description_df.job_type_target)):\n",
    "                target_vocab.add_token(job_type)\n",
    "            \n",
    "            \n",
    "        word_counts = Counter()\n",
    "        for description in description_df.job_description:\n",
    "            for token in description:\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        description_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                description_vocab.add_token(word)\n",
    "        \n",
    "        return cls(description_vocab, target_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        description_vocab = \\\n",
    "            SequenceVocabulary.from_serializable(contents['description_vocab'])\n",
    "        target_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['target_vocab'])\n",
    "\n",
    "        return cls(description_vocab=description_vocab, target_vocab=target_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'description_vocab': self.description_vocab.to_serializable(),\n",
    "                'target_vocab': self.target_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e1647b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_job_type = GloveVectorizer.from_dataframe(df, target_cat = False)\n",
    "glove_category = GloveVectorizer.from_dataframe(df, target_cat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f1221d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove_vectorizer_job_type.json\", \"w\") as fp:\n",
    "            json.dump(glove_job_type.to_serializable(), fp)\n",
    "        \n",
    "with open(\"glove_vectorizer_category.json\", \"w\") as fp:\n",
    "            json.dump(glove_category.to_serializable(), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51687eba",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3aa3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, encoding=\"utf8\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0661f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.10,\n",
    "    test_proportion=0.20,\n",
    "    output_munged_csv=\"description_with_splits.csv\",\n",
    "    seed=1337,\n",
    "    \n",
    "    # Data and Path hyper parameters\n",
    "    description_csv=\"description_with_splits.csv\",\n",
    "    vectorizer_file_one_hot_job_type=\"vectorizer_one_hot_job_type.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage\\document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='..glove.6B/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "   \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d95f889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage\\document_classification\\model_storage\\document_classification\\vectorizer.json\n",
      "\tmodel_storage\\document_classification\\model_storage\\document_classification\\model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a141b",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "de5efd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# Splitting train by category \n",
    "# Create dict\n",
    "by_category = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_category[row.category].append(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "622610b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split data\n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "for _, item_list in sorted(by_category.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_proportion*n)\n",
    "    n_val = int(args.val_proportion*n)\n",
    "    n_test = int(args.test_proportion*n)\n",
    "    \n",
    "    # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "    \n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "05a5bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write split data to file\n",
    "final_description = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "51565031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write munged data to CSV\n",
    "final_description.to_csv(args.output_munged_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87116c6",
   "metadata": {},
   "source": [
    "### Create dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "268b03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, vectorizer, short_description=True, job_type = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): the dataset\n",
    "            vectorizer (NewsVectorizer): vectorizer instatiated from dataset\n",
    "            short_description: indicates whether top 10 TFIDF words description\n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.short_description = short_description\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        \n",
    "        if self.short_description:\n",
    "            self._max_seq_length = max(map(measure_len, df.short_description)) + 2\n",
    "        else:\n",
    "            self._max_seq_length = max(map(measure_len, df.job_description)) + 2\n",
    "\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        if job_type:\n",
    "            class_counts = df.job_type_target.value_counts().to_dict()\n",
    "        else:\n",
    "            class_counts = df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.target_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, csv,category =True, one_hot = True, glove =False):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of NewsDataset\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv)\n",
    "        train_df = df[df.split=='train']\n",
    "        \n",
    "        if one_hot:\n",
    "            \n",
    "            return cls(df, One_hot_Vectorizer.from_dataframe(train_df, target_cat= category))\n",
    "        elif glove:\n",
    "            return cls(df, GloveVectorizer.from_dataframe(train_df, target_cat= category))\n",
    "        ### NEED TO INCLUDE WORD TO VEC\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of NewsDataset\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of NewsVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return Vectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        if self.short_description:\n",
    "            description_vector = \\\n",
    "                self._vectorizer.vectorize(row.short_description, self._max_seq_length)\n",
    "        else:\n",
    "            description_vector = \\\n",
    "                self._vectorizer.vectorize(row.job_description, self._max_seq_length)\n",
    "\n",
    "        target_index = \\\n",
    "            self._vectorizer.target_vocab.lookup_token(row.category)\n",
    "\n",
    "        return {'x_data': description_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73346e09",
   "metadata": {},
   "source": [
    "#### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "367664f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_one_hot_tfidf_job_type = Dataset(final_description, one_hot_encoder_job_type)\n",
    "dataset_one_hot_full_desc_job_type = Dataset(final_description,one_hot_encoder_job_type , short_description = False)\n",
    "dataset_glove_tfidf_job_type = Dataset(final_description, glove_job_type)\n",
    "dataset_glove_full_desc_job_type = Dataset(final_description,glove_job_type, short_description = False)\n",
    "#dataset.save_vectorizer(args.vectorizer_file)\n",
    "#dataloader = DataLoader(dataset=dataset, batch_size=2,\n",
    "#                            shuffle=True, drop_last=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4b4dc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset_one_hot_tfidf_job_type.pkl\", \"wb\") as outp:\n",
    "    pickle.dump(dataset_one_hot_tfidf_job_type, outp)\n",
    "   \n",
    "    \n",
    "with open(\"dataset_one_hot_full_desc_job_type.pkl\", \"wb\") as outp:\n",
    "    pickle.dump(dataset_one_hot_full_desc_job_type, outp)\n",
    "    \n",
    "with open(\"dataset_glove_tfidf_job_type.pkl\", \"wb\") as outp:\n",
    "    pickle.dump(dataset_glove_tfidf_job_type, outp)\n",
    "   \n",
    "    \n",
    "with open(\"dataset_glove_full_desc_job_type.pkl\", \"wb\") as outp:\n",
    "    pickle.dump(dataset_glove_full_desc_job_type, outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0e68fb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_type</th>\n",
       "      <th>short_description</th>\n",
       "      <th>job_type_target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>['today', 'we', 'have', 'around', 'people', 'w...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['we', 'to', 'and', 'the', 'we', 'to', 'we', '...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>['payroll', 'officer', 'super', 'global', 'org...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['payroll', 'officer', 'for', 'and', 'north', ...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>['work', 'with', 'australia', 'online', 'tax',...</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>['online', 'tax', 'and', 'online', 'tax', 'to'...</td>\n",
       "      <td>Other</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>['careers', 'candidates', 'has', 'been', 'enga...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['to', 'the', 'finance', 'and', 'the', 'the', ...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>['fantastic', 'opportunity', 'to', 'step', 'up...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['fantastic', 'to', 'manager', 'for', 'about',...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>['boyds', 'bay', 'landscaping', 'is', 'an', 'e...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['landscape', 'and', 'to', 'clients', 'and', '...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>['about', 'the', 'business', 'we', 'are', 'an'...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['of', 'mechanical', 'of', 'wa', 'mechanical',...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>['south', 'coast', 'trucks', 'machinery', 'cen...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['machinery', 'kobelco', 'asv', 'machinery', '...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>['gas', 'industry', 'field', 'operator', 'dist...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['gas', 'industry', 'field', 'operator', 'dist...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>['who', 'we', 'are', 'we', 'are', 'nsw', 'bulk...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>['nsw', 'water', 'river', 'and', 'water', 'and...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2963 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               category                                    job_description  \\\n",
       "0            Accounting  ['today', 'we', 'have', 'around', 'people', 'w...   \n",
       "1            Accounting  ['payroll', 'officer', 'super', 'global', 'org...   \n",
       "2            Accounting  ['work', 'with', 'australia', 'online', 'tax',...   \n",
       "3            Accounting  ['careers', 'candidates', 'has', 'been', 'enga...   \n",
       "4            Accounting  ['fantastic', 'opportunity', 'to', 'step', 'up...   \n",
       "...                 ...                                                ...   \n",
       "2958  Trades & Services  ['boyds', 'bay', 'landscaping', 'is', 'an', 'e...   \n",
       "2959  Trades & Services  ['about', 'the', 'business', 'we', 'are', 'an'...   \n",
       "2960  Trades & Services  ['south', 'coast', 'trucks', 'machinery', 'cen...   \n",
       "2961  Trades & Services  ['gas', 'industry', 'field', 'operator', 'dist...   \n",
       "2962  Trades & Services  ['who', 'we', 'are', 'we', 'are', 'nsw', 'bulk...   \n",
       "\n",
       "             job_type                                  short_description  \\\n",
       "0           Full Time  ['we', 'to', 'and', 'the', 'we', 'to', 'we', '...   \n",
       "1           Full Time  ['payroll', 'officer', 'for', 'and', 'north', ...   \n",
       "2     Casual/Vacation  ['online', 'tax', 'and', 'online', 'tax', 'to'...   \n",
       "3           Full Time  ['to', 'the', 'finance', 'and', 'the', 'the', ...   \n",
       "4           Full Time  ['fantastic', 'to', 'manager', 'for', 'about',...   \n",
       "...               ...                                                ...   \n",
       "2958        Full Time  ['landscape', 'and', 'to', 'clients', 'and', '...   \n",
       "2959        Full Time  ['of', 'mechanical', 'of', 'wa', 'mechanical',...   \n",
       "2960        Full Time  ['machinery', 'kobelco', 'asv', 'machinery', '...   \n",
       "2961        Full Time  ['gas', 'industry', 'field', 'operator', 'dist...   \n",
       "2962        Full Time  ['nsw', 'water', 'river', 'and', 'water', 'and...   \n",
       "\n",
       "     job_type_target  split  \n",
       "0          Full Time  train  \n",
       "1          Full Time  train  \n",
       "2              Other  train  \n",
       "3          Full Time  train  \n",
       "4          Full Time  train  \n",
       "...              ...    ...  \n",
       "2958       Full Time   test  \n",
       "2959       Full Time   test  \n",
       "2960       Full Time   test  \n",
       "2961       Full Time   test  \n",
       "2962       Full Time   test  \n",
       "\n",
       "[2963 rows x 6 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285831a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c89f1b53f5cd444e3157448a8cfaf5cd042e20f991203c5786aa4f19ac50ffc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
